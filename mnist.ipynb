{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVPYa9b_WjrB",
        "outputId": "7a212c55-8502-4cef-810e-34c9e3d6a9a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 44s 23ms/step - loss: 1.8030 - accuracy: 0.9407 - val_loss: 0.8763 - val_accuracy: 0.9714\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 34s 18ms/step - loss: 1.0934 - accuracy: 0.9673 - val_loss: 0.9097 - val_accuracy: 0.9746\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 34s 18ms/step - loss: 1.1376 - accuracy: 0.9694 - val_loss: 1.1962 - val_accuracy: 0.9686\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 36s 19ms/step - loss: 1.0094 - accuracy: 0.9734 - val_loss: 1.1510 - val_accuracy: 0.9710\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 34s 18ms/step - loss: 0.9595 - accuracy: 0.9748 - val_loss: 1.3024 - val_accuracy: 0.9716\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 34s 18ms/step - loss: 1.0853 - accuracy: 0.9760 - val_loss: 1.7739 - val_accuracy: 0.9652\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 34s 18ms/step - loss: 1.3071 - accuracy: 0.9752 - val_loss: 1.7482 - val_accuracy: 0.9692\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 34s 18ms/step - loss: 1.6174 - accuracy: 0.9717 - val_loss: 2.2579 - val_accuracy: 0.9616\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 35s 19ms/step - loss: 1.8636 - accuracy: 0.9683 - val_loss: 1.6137 - val_accuracy: 0.9724\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 34s 18ms/step - loss: 1.9952 - accuracy: 0.9665 - val_loss: 2.1466 - val_accuracy: 0.9644\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.8760 - accuracy: 0.9686\n",
            "Accuracy on test set: 0.9685999751091003\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape the data to have a fourth dimension\n",
        "X_train = X_train.reshape(-1, 28, 28, 1)\n",
        "X_test = X_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# Normalize the pixel values\n",
        "X_train = X_train.astype('float32') / 255\n",
        "X_test = X_test.astype('float32') / 255\n",
        "\n",
        "# One-hot encode the targets\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Split the test set into validation and test sets\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5)\n",
        "\n",
        "# Define the dynamic margin loss function\n",
        "batch_size = 32\n",
        "n_samples = len(X_train)\n",
        "\n",
        "def get_dynamic_margin_loss(optimizer, margin=0.25, scale=50.0):\n",
        "    class DynamicMarginLoss(tf.keras.losses.Loss):\n",
        "        def __init__(self, margin, scale):\n",
        "            super().__init__()\n",
        "            self.margin = margin\n",
        "            self.scale = scale\n",
        "\n",
        "        def call(self, y_true, y_pred):\n",
        "            epoch = tf.cast(optimizer.iterations, tf.float32) / tf.cast((n_samples / batch_size), tf.float32)\n",
        "            dynamic_margin = self.margin * (1 - 0.01 * epoch)\n",
        "            y_pred = y_true * (y_pred - dynamic_margin) + (1 - y_true) * y_pred\n",
        "            y_pred *= self.scale\n",
        "            return tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
        "    return DynamicMarginLoss(margin, scale)\n",
        "\n",
        "# Define the model architecture\n",
        "inputs = Input(shape=(28, 28, 1))\n",
        "x = Conv2D(32, kernel_size=(3, 3), activation='relu')(inputs)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "outputs = Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "# Compile the model\n",
        "initial_learning_rate = 0.01\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate,\n",
        "    decay_steps=100000,\n",
        "    decay_rate=0.96,\n",
        "    staircase=True)\n",
        "\n",
        "optimizer = Adam(learning_rate=lr_schedule)\n",
        "loss = get_dynamic_margin_loss(optimizer, margin=0.25, scale=50.0)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate the model\n",
        "results = model.evaluate(X_test, y_test)\n",
        "accuracy = results[1]\n",
        "\n",
        "print(f\"Accuracy on test set: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ceXxeC-hWk5i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}